{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30cac6c8",
   "metadata": {},
   "source": [
    "# M2 L1-Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f77fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.']\n",
      "Filtered Tokens: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', '.']\n",
      "Stemmed Tokens: ['natur', 'languag', 'process', '(', 'nlp', ')', 'subfield', 'linguist', ',', 'comput', 'scienc', ',', 'artifici', 'intellig', '.']\n",
      "Lemmatized Tokens: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords  # Fill in the blank\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer  # Fill in the blank\n",
    "from nltk.tokenize import word_tokenize  # Fill in the blank\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Text\n",
    "text = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer \n",
    "science, and artificial intelligence. \n",
    "It involves the interactions between computers and humans using the natural \n",
    "language. The ultimate objective \n",
    "of NLP is to read, decipher, understand, and make sense of the human language in\n",
    "a valuable way. It started \n",
    "in the 1950s, although work can be found from earlier periods. In 1950, Alan \n",
    "Turing published an article titled \n",
    "\"Computing Machinery and Intelligence\" which proposed what is now called the \n",
    "Turing test as a criterion of \n",
    "intelligence, a task that involves the automated interpretation and generation \n",
    "of natural language, but at the \n",
    "time not articulated as a problem separate from artificial intelligence. The \n",
    "premise of symbolic NLP is \n",
    "well-summarized by John Searle's Chinese room experiment: Given a collection of \n",
    "rules (e.g., a Chinese phrasebook, \n",
    "with questions and matching answers), the computer emulates natural language \n",
    "understanding (or other NLP tasks) \n",
    "by applying those rules to the data it is confronted with. 2023 is the year when\n",
    "NLP got its major breakthrough.\n",
    "\"\"\"\n",
    "\n",
    "# Task 1: Tokenization\n",
    "# Write a function to tokenize the text and return the tokens for first line in the paragraph\n",
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    first_line = sentences[0]\n",
    "    tokens = word_tokenize(first_line)\n",
    "    return tokens\n",
    "\n",
    "# Task 2: Stop Word Removal\n",
    "# Write a function to remove stop words for the whole paragraph from the tokens and return the filtered tokens\n",
    "def remove_stop_words(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Task 3: Stemming\n",
    "# Write a function to perform stemming on the filtered tokens and return the stemmed tokens\n",
    "def perform_stemming(filtered_tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "# Task 4: Lemmatization\n",
    "# Write a function to perform lemmatization on the filtered tokens and return the lemmatized tokens\n",
    "def perform_lemmatization(filtered_tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Now use the functions to process the text\n",
    "tokens = tokenize_text(text)\n",
    "filtered_tokens = remove_stop_words(tokens)\n",
    "stemmed_tokens = perform_stemming(filtered_tokens)\n",
    "lemmatized_tokens = perform_lemmatization(filtered_tokens)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Filtered Tokens:\", filtered_tokens)\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae214fc",
   "metadata": {},
   "source": [
    "# M2 L2-Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 1: Co-occurrence Matrix\n",
    "To create a co-occurrence (word-word) matrix for the given sentences, we need to determine the context in which words co-occur. First, let's define the co-occurrence matrix using the context of a sentence. Then, we'll redefine it using a window of three words (one on the left and one on the right).\n",
    "\n",
    "Sentences:\n",
    "'Apples are green and red.'\n",
    "'Red apples are sweet.'\n",
    "'Green oranges are sour.'\n",
    "Context: Sentence\n",
    "The words are: apples, are, green, and, red, sweet, oranges, sour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e22a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "Co-occurrence Matrix (Context: Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d0b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "         apples   are  green    and    red  sweet  oranges  sour\n",
    "apples        0     2      1      1      1      1        0     0\n",
    "are           2     0      2      1      1      1        1     1\n",
    "green         1     2      0      1      1      0        1     1\n",
    "and           1     1      1      0      1      0        0     0\n",
    "red           1     1      1      1      0      1        0     0\n",
    "sweet         1     1      0      0      1      0        0     0\n",
    "oranges       0     1      1      0      0      0        0     1\n",
    "sour          0     1      1      0      0      0        1     0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f8ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Co-occurrence Matrix (Context: Window of Three Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5ff8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "         apples   are  green    and    red  sweet  oranges  sour\n",
    "apples        0     1      1      0      1      1        0     0\n",
    "are           1     0      1      1      1      1        1     0\n",
    "green         1     1      0      1      0      0        1     1\n",
    "and           0     1      1      0      1      0        0     0\n",
    "red           1     1      0      1      0      1        0     0\n",
    "sweet         1     1      0      0      1      0        0     0\n",
    "oranges       0     1      1      0      0      0        0     1\n",
    "sour          0     0      1      0      0      0        1     0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333edc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 2: TF-IDF Calculation\n",
    "First, let's manually calculate TF-IDF for the provided corpus.\n",
    "\n",
    "Corpus:\n",
    "\"the cat sat on the mat\"\n",
    "\"the dog sat on the log\"\n",
    "\"cats and dogs are great\"\n",
    "Step-by-Step Calculation\n",
    "1. Term Frequency (TF)\n",
    "TF for each word in each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fba092",
   "metadata": {},
   "outputs": [],
   "source": [
    "Document 1: {'the': 2/6, 'cat': 1/6, 'sat': 1/6, 'on': 1/6, 'mat': 1/6}\n",
    "Document 2: {'the': 2/6, 'dog': 1/6, 'sat': 1/6, 'on': 1/6, 'log': 1/6}\n",
    "Document 3: {'cats': 1/5, 'and': 1/5, 'dogs': 1/5, 'are': 1/5, 'great': 1/5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a58da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Inverse Document Frequency (IDF)\n",
    "IDF for each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b4276",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF = log(N / df) where N is the total number of documents and df is the number of documents containing the word.\n",
    "\n",
    "the: log(3/2)\n",
    "cat: log(3/1)\n",
    "sat: log(3/2)\n",
    "on: log(3/2)\n",
    "mat: log(3/1)\n",
    "dog: log(3/1)\n",
    "log: log(3/1)\n",
    "cats: log(3/1)\n",
    "and: log(3/1)\n",
    "dogs: log(3/1)\n",
    "are: log(3/1)\n",
    "great: log(3/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6adf417",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. TF-IDF Calculation\n",
    "TF-IDF for each word in each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dedfc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Document 1: {'the': TF*IDF, 'cat': TF*IDF, 'sat': TF*IDF, 'on': TF*IDF, 'mat': TF*IDF}\n",
    "Document 2: {'the': TF*IDF, 'dog': TF*IDF, 'sat': TF*IDF, 'on': TF*IDF, 'log': TF*IDF}\n",
    "Document 3: {'cats': TF*IDF, 'and': TF*IDF, 'dogs': TF*IDF, 'are': TF*IDF, 'great': TF*IDF}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1befa1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 12\n",
      "The words in the corpus:\n",
      " {'log', 'great', 'are', 'sat', 'dog', 'on', 'mat', 'and', 'cats', 'dogs', 'the', 'cat'}\n",
      "TF Matrix:\n",
      "        log  great  are       sat       dog        on       mat  and  cats  \\\n",
      "0  0.000000    0.0  0.0  0.166667  0.000000  0.166667  0.166667  0.0   0.0   \n",
      "1  0.166667    0.0  0.0  0.166667  0.166667  0.166667  0.000000  0.0   0.0   \n",
      "2  0.000000    0.2  0.2  0.000000  0.000000  0.000000  0.000000  0.2   0.2   \n",
      "\n",
      "   dogs       the       cat  \n",
      "0   0.0  0.333333  0.166667  \n",
      "1   0.0  0.333333  0.000000  \n",
      "2   0.2  0.000000  0.000000  \n",
      "IDF of: \n",
      "            log: 0.47712125471966244\n",
      "          great: 0.47712125471966244\n",
      "            are: 0.47712125471966244\n",
      "            sat: 0.17609125905568124\n",
      "            dog: 0.47712125471966244\n",
      "             on: 0.17609125905568124\n",
      "            mat: 0.47712125471966244\n",
      "            and: 0.47712125471966244\n",
      "           cats: 0.47712125471966244\n",
      "           dogs: 0.47712125471966244\n",
      "            the: 0.17609125905568124\n",
      "            cat: 0.47712125471966244\n",
      "TF-IDF Matrix:\n",
      "       log     great       are       sat      dog        on      mat  \\\n",
      "0  0.00000  0.000000  0.000000  0.029349  0.00000  0.029349  0.07952   \n",
      "1  0.07952  0.000000  0.000000  0.029349  0.07952  0.029349  0.00000   \n",
      "2  0.00000  0.095424  0.095424  0.000000  0.00000  0.000000  0.00000   \n",
      "\n",
      "        and      cats      dogs       the      cat  \n",
      "0  0.000000  0.000000  0.000000  0.058697  0.07952  \n",
      "1  0.000000  0.000000  0.000000  0.058697  0.00000  \n",
      "2  0.095424  0.095424  0.095424  0.000000  0.00000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### Document\n",
    "corpus = [\"the cat sat on the mat\", \"the dog sat on the log\", \"cats and dogs are great\"]\n",
    "\n",
    "## Word set of the corpus\n",
    "words_set = set()\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    words_set = words_set.union(set(words))\n",
    "\n",
    "print('Number of words in the corpus:', len(words_set))\n",
    "print('The words in the corpus:\\n', words_set)\n",
    "\n",
    "#### TF\n",
    "n_docs = len(corpus)  # Number of documents in the corpus\n",
    "n_words_set = len(words_set)  # Number of unique words in the corpus\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=list(words_set))\n",
    "\n",
    "# Compute Term Frequency (TF)\n",
    "for i in range(n_docs):\n",
    "    words = corpus[i].split(' ')  # Words in the document\n",
    "    for w in words:\n",
    "        df_tf.loc[i, w] += 1 / len(words)\n",
    "\n",
    "print(\"TF Matrix:\")\n",
    "print(df_tf)\n",
    "\n",
    "### Compute IDF\n",
    "print(\"IDF of: \")\n",
    "idf = {}\n",
    "for w in words_set:\n",
    "    k = 0  # number of documents in the corpus that contain this word\n",
    "    for i in range(n_docs):\n",
    "        if w in corpus[i].split(' '):\n",
    "            k += 1\n",
    "    idf[w] = np.log10(n_docs / k)\n",
    "    print(f'{w:>15}: {idf[w]:>10}')\n",
    "\n",
    "### Compute TF-IDF\n",
    "df_tfidf = df_tf.copy()\n",
    "for w in words_set:\n",
    "    df_tfidf[w] = df_tf[w] * idf[w]\n",
    "\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(df_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6827449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d2937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f62f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sbai)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
